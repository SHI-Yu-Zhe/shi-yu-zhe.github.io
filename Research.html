<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Research</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="wrapper style2-black fullscreen fade-up">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Yuzhe Shi</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<!--<li><a href="generic.html" class="active">Generic</a></li>
						<li><a href="elements.html">Elements</a></li>-->
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Research</h1>
							<!--<span class="image fit"><img src="images/pic04.jpg" alt="" /></span>-->
							<div class="features">
								<section>
									<span class="icon solid major fa-code"></span>
									<h2>Human-Level Abuctive Learning and Planning</h2>
									<p>Key Words: Neural-Symbolic Learning, Abductive Reasoning, Qualitative Simulation, Inverse Planning.</p>
									<p>Motivation: A novice Minecraft player can explore the world well from only visual observation given very little guidance thanks to the rich background knowledge (or commonsense). Can a machine learner do the same?</p>
									<p>Task: I prepare several first-person video sequences recording a rational human player playing Minecraft and define some logic rules representing the human commonsense, e.g. Perspective Relationship.</p>
									<p>Method: The learner models the agent operations following the idea of Qualitative Simulation, which describes the motion not step-to-step but in a high-level state-to-state way, yielding greater representative power. It exploits a Transformer model to translate the video sequence to a sequence of motion state transition signals. Then the learner tries to inference subgoals of the player by abducting an interpretation from motion observations and background knowledge. Finally, the learner generates logic programs representing the strategies learned from human players.</p>
									<ul class="actions">
										<li><a href="https://github.com/YuzheSHI/ABLaoi" class="button scrolly">Project</a></li>
									</ul>
								</section>
								<section>
									<span class="icon solid major fa-code"></span>
									<h2>Interpretative Neural Feature Primitives for Image Classification</h2>
									<p>Key Words: Key Words: Explanability of Neural Networks, Abductive Reasoning, Visual Association.</p>
									<p>Motivation: Human can recognize instances by executing association between different visual concepts using simple features such as shape, texture, color or symbol. I call these simple explanable features primitives. Can we train a neural network that detects such feature primmitives, and has compositional generalization ability over feature primitives?</p>
									<p>Toy Data: We construct a toy image dataset with objects in diverse shapes, textures, colors, and with different symbols on them, using C4D.</p>
									<p>Method: We train 4 CNN classifiers, one in each simple feature family. For a particular sample, we order the other samples by calculating their similarities to it. Experiments show that our method extremely reduces the open-world risk for novel instances and we can link them with known classes by applying association over some features. We couldn't go further due to the lack of computing resource.</p>
									<ul class="actions">
										<li><a href="https://github.com/YuzheSHI/ABL-Feature" class="button scrolly">Project</a></li>
									</ul>
								</section>
								<section>
									<span class="icon solid major fa-code"></span>
									<h2>Waiting for the Bus: A Human-Centered Computing Perspective</h2>
									<p>Key Words: Subjective Probability, Inverse Planning.</p>
									<p>Motivation: How do long-term and short-term perspective affect human subjective probability of waiting for a bus? Can we design a intelligent bot that helps users feel better when waiting for some event?</p>
									<p>Human Behavior Experiments:: We design a $2\times 6$ experiment making participants waiting for a schoolbus in a virtual environment simulating the campus, within 6 scenes and 2 ways of prompts (in short-term and long-term respectively). We record the videos and interactions with the computer of 28 participants.</p>
									<p>Method: We apply micro-expression recognition models to the videos as groundtruth of emotion states of the participants and develop a bayesian model inferencing emotion states from human interactions with the computer via inverse planning. We aim to develop a bot that selects prompting strategy adaptively for different users according to their behavior on the interface.</p>
									<ul class="actions">
										<li><a href="https://github.com/YuzheSHI/Waiting-Interaction-Experiment" class="button scrolly">Project</a></li>
									</ul>
								</section>
								<section>
									<span class="icon solid major fa-code"></span>
									<h2>Abductive Novel Object Invention for Incremental Learning</h2>
									<p>Key Words: Neural-Symbolic Learning, Probabilistic CFG, Expectation Maximization.</p>
									<p>Motivation: Can the learner detect instances belonging to classes that never been seen nor known before from raw data and label them autonomously with the help of background knowledge? </p>
									<p>Task: The learner starts with a CNN classifier that recognizes hand-written symbols \texttt{0, 1, 2, +, -, =} and background knowledge about successor relation.</p>
									<p>Method: We feed the learner with image sequences like \texttt{0+++=3} and it tries to estimate the distribution of novel objects by both the classification score and logical consistency of PCFG. Then the learner abducts an atomic predicate representing the relationship between the novel class and known classes, e.g. \texttt{new\_digit(X):-succ(2,X)}. It labels the novel instances with the predicates and update the classifier.</p>
									<ul class="actions">
										<li><a href="https://github.com/YuzheSHI/ABL-AOI" class="button scrolly">Project</a></li>
									</ul>
								</section>
								<section>
									<span class="icon solid major fa-code"></span>
									<h2>Experimental Study on Visual Object Tracking</h2>
									<p>We study the merits of diverse visual tracking paradigms by comparative experiments. Having learned the limitations of deep learning, I decided to pursuit for intelligence which is more general, reliable and comprehensible.</p>
									<ul class="actions">
										<li><a href="https://github.com/YuzheSHI/CSUR-paper-workspace" class="button scrolly">Project</a></li>
									</ul>
								</section>
							</div>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Yuzhe Shi. All rights reserved.</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
